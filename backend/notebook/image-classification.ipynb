{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756d16a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import pickle\n",
    "import sys\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "sys.path.append(\"../../backend\")\n",
    "\n",
    "from image_encoder import ImageEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f1abb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"D:/projects/fyndart/artworks/\"\n",
    "MODEL_PATH = \"../deep_encoder.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28dea213",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
    "STD = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "T1 = Resize(224, interpolation=Image.BICUBIC)\n",
    "T2 = CenterCrop(224)\n",
    "T3 = lambda x: x.convert(\"RGB\")\n",
    "T4 = ToTensor()\n",
    "T5 = Normalize(MEAN, STD)\n",
    "\n",
    "def to_tensor(images):\n",
    "    return torch.cat([Compose([T1, T2, T3, T4, T5])(image).unsqueeze(0) for image in images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b4e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = []\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "encoder = ImageEncoder(MODEL_PATH, device)\n",
    "\n",
    "collection1 = glob.glob(DATA_PATH + '*.jpeg')\n",
    "collection2 = glob.glob(DATA_PATH + '*.jpg')\n",
    "for file in (collection1 + collection2):\n",
    "    references.append(file)\n",
    "\n",
    "chunk_size = 100\n",
    "chunks = [references[i:i + chunk_size] for i in range(0, len(references), chunk_size)]\n",
    "\n",
    "references = list(map(lambda x: x.replace(\"\\\\\", \"/\"), references))\n",
    "references = list(map(lambda x: x[x.rfind(\"/\") + 1:], references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbade87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process chunk 0\n",
      "process chunk 1\n",
      "process chunk 2\n",
      "process chunk 3\n",
      "process chunk 4\n",
      "process chunk 5\n",
      "process chunk 6\n",
      "process chunk 7\n",
      "process chunk 8\n",
      "process chunk 9\n",
      "process chunk 10\n",
      "process chunk 11\n",
      "process chunk 12\n",
      "process chunk 13\n",
      "process chunk 14\n",
      "process chunk 15\n",
      "process chunk 16\n",
      "process chunk 17\n",
      "process chunk 18\n",
      "process chunk 19\n",
      "process chunk 20\n",
      "process chunk 21\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "for index, chunk in enumerate(chunks):\n",
    "    images = list(map(lambda x: Image.open(x), chunk))\n",
    "    print(\"process chunk \" + str(index))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tensor = to_tensor(images).to(device)\n",
    "        for embedding in encoder(tensor).cpu().detach().numpy():\n",
    "            embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4cf96b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('embeddings.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'references': references, 'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67395919",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    references = stored_data['references']\n",
    "    embeddings = stored_data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24d87d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 7, 7])\n",
      "torch.Size([1, 256, 7, 7])\n",
      "tensor([[[0.7691, 0.7978, 0.7055, 0.5842, 0.6243, 0.5780, 0.4434],\n",
      "         [0.7251, 0.6972, 0.6040, 0.7631, 0.7437, 0.5954, 0.5275],\n",
      "         [0.6077, 0.6612, 0.7934, 0.7069, 0.7144, 0.6128, 0.5885],\n",
      "         [0.6952, 0.7574, 0.7730, 0.6081, 0.6449, 0.6190, 0.6928],\n",
      "         [0.5181, 0.6390, 0.8613, 0.7970, 0.7591, 0.7614, 0.6051],\n",
      "         [0.7512, 0.6886, 0.8427, 0.8029, 0.7736, 0.5426, 0.4037],\n",
      "         [0.5948, 0.4834, 0.8211, 0.9257, 0.9315, 0.7774, 0.5763]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tensor1 = to_tensor([Image.open(collection1[0])]).to(device)\n",
    "tensor2 = to_tensor([Image.open(collection1[1])]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding1 = encoder(tensor1)\n",
    "    embedding2 = encoder(tensor2)\n",
    "\n",
    "print(embedding1.shape)\n",
    "print(embedding2.shape)\n",
    "    \n",
    "from torch.nn import CosineSimilarity\n",
    "cos = CosineSimilarity()\n",
    "print(cos(embedding1, embedding2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82bdcbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 7, 7])\n",
      "torch.Size([2123, 256, 7, 7])\n",
      "torch.Size([1, 256, 7, 7])\n",
      "torch.Size([2123, 256, 7, 7])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "self must be a matrix",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-6e92fed7f3e0>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     27\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma_norm\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb_norm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m \u001B[0mcos_scores\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcos_sim\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage_embedding\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membeddings\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m \u001B[0mtop_results\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtopk\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcos_scores\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mk\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-8-6e92fed7f3e0>\u001B[0m in \u001B[0;36mcos_sim\u001B[1;34m(a, b)\u001B[0m\n\u001B[0;32m     25\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mb_norm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma_norm\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mb_norm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[0mcos_scores\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcos_sim\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage_embedding\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membeddings\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: self must be a matrix"
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "cos_scores = cos_sim(image_embedding, embeddings)[0]\n",
    "top_results = torch.topk(cos_scores, k=5)\n",
    "\n",
    "preds = top_results.values.tolist()\n",
    "index = top_results.indices.tolist()\n",
    "\n",
    "for result in zip(preds, index):\n",
    "    print(f\"({round(result[0], 4)}) {references[result[1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d879e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5bab1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visual-transformer",
   "language": "python",
   "name": "visual-transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}